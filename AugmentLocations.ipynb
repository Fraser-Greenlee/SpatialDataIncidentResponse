{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AugmentLocations.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "rCISTltGwmKb",
        "collapsed": true,
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Mount Google Drive (Run each time)\n",
        "%%capture\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\", force_remount = True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Create Google Drive directories (Run at first setup)\n",
        "%%capture\n",
        "\n",
        "import os\n",
        "os.mkdir(\"/content/drive/MyDrive/SpatialData\")\n",
        "os.mkdir(\"/content/drive/MyDrive/SpatialData/ExternalData\")\n",
        "os.mkdir(\"/content/drive/MyDrive/SpatialData/Processed\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "eYYRCXI3JRVY"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Upload files to Google Drive (Upload at first setup and as required if they cannot be updated within Google Drive)\n",
        "\n",
        "- MyDrive/SpatialData/**APs_Master.ods**\n",
        "- MyDrive/SpatialData/**RVs_Master.ods**\n",
        "- MyDrive/SpatialData/ExternalData/**codepo_gpkg_gb.zip** (https://osdatahub.os.uk/downloads/open/CodePointOpen)\n",
        "- MyDrive/SpatialData/ExternalData/**oproad_gpkg_gb.zip** (https://osdatahub.os.uk/downloads/open/OpenRoads)"
      ],
      "metadata": {
        "id": "JUbnuOOfiIGf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Unzip Code Point and Open Roads data (Run at first setup and as required to update)\n",
        "%%capture\n",
        "\n",
        "# Clean out the directories if they already exist\n",
        "!rm -r /content/drive/MyDrive/SpatialData/ExternalData/codepo_gpkg_gb\n",
        "!rm -r /content/drive/MyDrive/SpatialData/ExternalData/oproad_gpkg_gb\n",
        "\n",
        "# Unzip data contents\n",
        "!unzip /content/drive/MyDrive/SpatialData/ExternalData/codepo_gpkg_gb.zip -d /content/drive/MyDrive/SpatialData/ExternalData/codepo_gpkg_gb\n",
        "!unzip /content/drive/MyDrive/SpatialData/ExternalData/oproad_gpkg_gb.zip -d /content/drive/MyDrive/SpatialData/ExternalData/oproad_gpkg_gb"
      ],
      "metadata": {
        "id": "53iEr1IRyMSJ",
        "collapsed": true,
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Install dependencies (Run each time)\n",
        "%%capture\n",
        "\n",
        "!pip install geopandas\n",
        "!pip install gpxpy\n",
        "!pip install odfpy\n",
        "!sudo apt -y install libspatialindex-dev\n",
        "!pip install rtree"
      ],
      "metadata": {
        "id": "l_keqHJk1xPH",
        "collapsed": true,
        "cellView": "form"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Copyright (C) Edward Alan Lockhart 2022\n",
        "\n",
        "This program is free software: you can redistribute it and/or modify it under\n",
        "the terms of the GNU General Public License as published by the Free Software\n",
        "Foundation, either version 3 of the License, or (at your option) any later\n",
        "version.\n",
        "\n",
        "This program is distributed in the hope that it will be useful, but WITHOUT\n",
        "ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS\n",
        "FOR A PARTICULAR PURPOSE. See the GNU General Public License for more details.\n",
        "\n",
        "You should have received a copy of the GNU General Public License along with\n",
        "this program. If not, see https://www.gnu.org/licenses/.\n",
        "\n",
        "Description:\n",
        "    The purpose of this software is to ingest spreadsheets containing spatial\n",
        "    records of cave and mine Access Points (APs) and Rendezvous points (RVs),\n",
        "    to accurately and automatically convert these into a variety of location\n",
        "    formats, and add useful metadata from free external datasets and services.\n",
        "    The data are exported in a variety of standard formats.\n",
        "\n",
        "Column list:\n",
        "    Required in spreadsheet (additional columns are unaltered):\n",
        "        ID - Unique identifier\n",
        "        Name - Location name\n",
        "        LongLat - Longitude, Latitude in decimal degrees (WGS84)\n",
        "        VerifiedDate - Date of visit to verify location\n",
        "    Generated for RVs only:\n",
        "        RoadAccessType - Nearest road type\n",
        "        Postcode - Nearest postcode\n",
        "        MobileCoverage - Minimum outdoor mobile phone coverage\n",
        "    Generated for all locations:\n",
        "        Longitude - Decimal degrees (WGS84)\n",
        "        Latitude - Decimal degrees (WGS84)\n",
        "        Verified - Binary flag if checked\n",
        "        Easting - Metres (British National Grid)\n",
        "        Northing - Metres (British National Grid)\n",
        "        OSGridRef1m - Ordnance Survey 1m grid reference\n",
        "        What3Words - What3Words address\n",
        "        GoogleMapsURL - Google Maps link for directions and Street View\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "# User-defined variables\n",
        "ap_filepath = \"/content/drive/MyDrive/SpatialData/APs_Master.ods\" # APs spreadsheet filepath\n",
        "rv_filepath = \"/content/drive/MyDrive/SpatialData/RVs_Master.ods\" # RVs spreadsheet filepath\n",
        "codepoint_filepath = \"/content/drive/MyDrive/SpatialData/ExternalData/codepo_gpkg_gb/data/codepo_gb.gpkg\" # Code Point filepath (https://osdatahub.os.uk/downloads/open/CodePointOpen)\n",
        "openroads_filepath = \"/content/drive/MyDrive/SpatialData/ExternalData/oproad_gpkg_gb/data/oproad_gb.gpkg\" # Open Roads filepath (https://osdatahub.os.uk/downloads/open/OpenRoads)\n",
        "export_directory = \"/content/drive/MyDrive/SpatialData/Processed\" # Any directory to export the processed files to\n",
        "ofcom_api_key = '#####' # Paste your API key here (https://api.ofcom.org.uk/products/mobile-premium)\n",
        "what3words_api_key = '#####' # Paste your API key here (https://developer.what3words.com/public-api)\n",
        "rv_gpx_symbology = 'None' # Optional - Replace with the symbology text string that is unique to the GPS device\n",
        "ap_gpx_symbology = 'None' # Optional - Replace with the symbology text string that is unique to the GPS device\n",
        "\n",
        "\n",
        "\n",
        "import os\n",
        "import time\n",
        "import http.client\n",
        "import urllib.parse\n",
        "import json\n",
        "import warnings\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import geopandas as gpd\n",
        "import gpxpy\n",
        "import gpxpy.gpx as g\n",
        "from datetime import datetime\n",
        "from scipy.spatial import cKDTree\n",
        "from shapely.ops import unary_union\n",
        "from shapely.geometry import LineString\n",
        "\n",
        "\n",
        "\n",
        "def get_coverage(postcode):\n",
        "    key = ofcom_api_key\n",
        "    api_url_root = \"api-proxy.ofcom.org.uk\"\n",
        "    fixed_url = \"/mobile/coverage/\"\n",
        "    params = urllib.parse.urlencode({})\n",
        "    headers = {\"Ocp-Apim-Subscription-Key\": key}\n",
        "    try:\n",
        "        conn = http.client.HTTPSConnection(api_url_root)\n",
        "        conn.request('GET', fixed_url + postcode + \"?%s\" % params, \"{body}\", headers)\n",
        "        response = conn.getresponse()\n",
        "        data = json.loads(response.read())\n",
        "        conn.close()\n",
        "        if 'Error' in data.keys():\n",
        "            raise Exception\n",
        "        return [address for address in data['Availability']]\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "def get_what3words(lat, long):\n",
        "    key = what3words_api_key\n",
        "    api_url_root = \"api.what3words.com\"\n",
        "    fixed_url = \"/v3/convert-to-3wa?coordinates=\"\n",
        "    try:\n",
        "        conn = http.client.HTTPSConnection(api_url_root)\n",
        "        conn.request('GET', fixed_url + str(lat) + \"%2C\" + str(long) + \"&key=\" + key)\n",
        "        response = conn.getresponse()\n",
        "        data = json.loads(response.read())\n",
        "        conn.close()\n",
        "        return data['words']\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "def point_buffer(points, distance, crs):\n",
        "    buffer = points.buffer(distance)\n",
        "    return gpd.GeoSeries(unary_union(buffer), crs = crs)\n",
        "\n",
        "def get_nearest(gdA, gdB):\n",
        "    nA = np.array(list(gdA.geometry.apply(lambda x: (x.x, x.y))))\n",
        "    nB = np.array(list(gdB.geometry.apply(lambda x: (x.x, x.y))))\n",
        "    btree = cKDTree(nB)\n",
        "    dist, idx = btree.query(nA, k = 1)\n",
        "    gdB_nearest = gdB.iloc[idx].drop(columns = 'geometry').reset_index(drop = True)\n",
        "    gdf = pd.concat([gdA.reset_index(drop = True),\n",
        "                     gdB_nearest,\n",
        "                     pd.Series(dist, name = 'dist')], axis = 1)\n",
        "    return gdf\n",
        "\n",
        "def redistribute_vertices(geom, distance):\n",
        "    if geom.geom_type == 'LineString':\n",
        "        num_vert = int(round(geom.length/distance))\n",
        "        if num_vert == 0:\n",
        "            num_vert = 1\n",
        "        return LineString([geom.interpolate(float(n)/num_vert, normalized = True) for n in range(num_vert + 1)])\n",
        "    elif geom.geom_type == 'MultiLineString':\n",
        "        parts = [redistribute_vertices(part, distance) for part in geom]\n",
        "        return type(geom)([p for p in parts if not p.is_empty])\n",
        "    else:\n",
        "        raise ValueError('Unhandled geometry ' + geom.geom_type)\n",
        "\n",
        "def xy_to_osgb(easting, northing, precision = 1):\n",
        "    major = {0: {0: 'S', 1: 'N', 2: 'H'},\n",
        "             1: {0: 'T', 1: 'O'}}\n",
        "    minor = {0: {0: 'V', 1: 'Q', 2: 'L', 3: 'F', 4: 'A'},\n",
        "             1: {0: 'W', 1: 'R', 2: 'M', 3: 'G', 4: 'B'},\n",
        "             2: {0: 'X', 1: 'S', 2: 'N', 3: 'H', 4: 'C'},\n",
        "             3: {0: 'Y', 1: 'T', 2: 'O', 3: 'J', 4: 'D'},\n",
        "             4: {0: 'Z', 1: 'U', 2: 'P', 3: 'K', 4: 'E'}}\n",
        "    \n",
        "    if precision not in [100000, 10000, 1000, 100, 10, 1]:\n",
        "        raise Exception('Precision of ' + str(precision) + ' is not supported')\n",
        "    \n",
        "    try:\n",
        "        x_idx = easting // 500000\n",
        "        y_idx = northing // 500000\n",
        "        major_letter = major[x_idx][y_idx]\n",
        "        macro_easting = easting % 500000\n",
        "        macro_northing = northing % 500000\n",
        "        macro_x_idx = macro_easting // 100000\n",
        "        macro_y_idx = macro_northing // 100000\n",
        "        minor_letter = minor[macro_x_idx][macro_y_idx]\n",
        "    except (ValueError, IndexError, KeyError, AssertionError):\n",
        "        raise Exception('Out of range')\n",
        "    \n",
        "    micro_easting = macro_easting % 100000\n",
        "    micro_northing = macro_northing % 100000\n",
        "    ref_x = micro_easting // precision\n",
        "    ref_y = micro_northing // precision\n",
        "\n",
        "    coord_width = 0\n",
        "    if precision == 10000:\n",
        "        coord_width = 1\n",
        "    elif precision == 1000:\n",
        "        coord_width = 2\n",
        "    if precision == 100:\n",
        "        coord_width = 3\n",
        "    elif precision == 10:\n",
        "        coord_width = 4\n",
        "    elif precision == 1:\n",
        "        coord_width = 5\n",
        "\n",
        "    format_string = (r\"%s%s %0\" + str(coord_width) + r\"d %0\" +\n",
        "                     str(coord_width) + r\"d\") if precision else r\"%s%s %0\"\n",
        "    return format_string % (major_letter, minor_letter, ref_x, ref_y)\n",
        "\n",
        "\n",
        "\n",
        "# Check that all filepaths and directories exist\n",
        "for path in [ap_filepath, rv_filepath, codepoint_filepath,\n",
        "             openroads_filepath, export_directory]:\n",
        "    if not os.path.exists(path):\n",
        "        raise Exception('Path ' + str(path) + ' does not exist')\n",
        "\n",
        "# Check transformations\n",
        "import pyproj\n",
        "tg = pyproj.transformer.TransformerGroup(27700, 4326)\n",
        "descriptions = \"\\n\\n\".join([str(i) for i in tg.transformers])\n",
        "accuracy = min([i.accuracy for i in tg.transformers if i.accuracy > 0])\n",
        "print('Transformation accuracy is', accuracy, 'm')\n",
        "if \"OSTN15_NTv2\" not in descriptions:\n",
        "    print(\"The OSTN15_NTv2 transformation is not present. To achieve a higher accuracy, extract the files in https://ordnancesurvey.co.uk/documents/resources/OSTN15-NTv2.zip to \" + pyproj.datadir.get_data_dir())\n",
        "\n",
        "    \n",
        "\n",
        "# Timestamp\n",
        "date = datetime.now().strftime(\"%Y-%m-%d\")\n",
        "\n",
        "# Loop through each location type and spreadsheet\n",
        "for location_type, path in zip(['APs', 'RVs'], [ap_filepath, rv_filepath]):\n",
        "    \n",
        "    print(\"\\nProcessing\", location_type)\n",
        "    \n",
        "    # Read in the master spreadsheet\n",
        "    data = pd.read_excel(path, engine = 'odf')\n",
        "    \n",
        "    \n",
        "    \n",
        "    print(\" - Formatting Inputs\")\n",
        "    # Check for blanks in essential columns\n",
        "    for i in ['ID', 'Name', 'LongLat']:\n",
        "        if data[i].isna().sum() > 0:\n",
        "            raise Exception(i + ' contains blank values')\n",
        "    \n",
        "    # Set column data types\n",
        "    try:\n",
        "        for i in ['Name', 'ID']:\n",
        "            data[i] = data[i].astype(str)\n",
        "    except:\n",
        "        raise Exception(i + ' contains non-string values')\n",
        "    try:\n",
        "            data['VerifiedDate'] = pd.to_datetime(data['VerifiedDate'])\n",
        "    except:\n",
        "        raise Exception('VerifiedDate contains non-datetime values')\n",
        "    try:\n",
        "        data[['Longitude', 'Latitude']] = data['LongLat'].str.split(\",\", expand = True)\n",
        "        del data['LongLat']\n",
        "        for i in ['Longitude', 'Latitude']:\n",
        "            data[i] = data[i].astype(float)\n",
        "    except:\n",
        "        raise Exception(\"LongLat does not contain comma-separated numeric values\")\n",
        "    \n",
        "    # Check for duplicate IDs\n",
        "    if len(data['ID']) != len(set(data['ID'])):\n",
        "        raise Exception('IDs are not unique')\n",
        "        \n",
        "    # Concatenate the name and ID columns, removing the latter\n",
        "    data['Name'] = data['Name'].astype(str) + \" (\" + data.pop('ID').astype(str) + \")\"\n",
        "    \n",
        "    \n",
        "    \n",
        "    print(\" - Creating Backup\")\n",
        "    # Export a timestamped CSV backup to the spreadsheet directory\n",
        "    data.to_csv(os.path.join(os.path.dirname(path),\n",
        "                             location_type + \"_Backup_\" + date + \".csv\"),\n",
        "                index = False)\n",
        "\n",
        "\n",
        "\n",
        "    print(\" - Adding Geometry\")\n",
        "    # Add geometry, set as WGS84\n",
        "    data = gpd.GeoDataFrame(data,\n",
        "                            crs = \"EPSG:4326\",\n",
        "                            geometry = gpd.points_from_xy(data['Longitude'],\n",
        "                                                          data['Latitude']))\n",
        "    # Convert to BNG\n",
        "    data = data.to_crs(\"EPSG:27700\")\n",
        "    \n",
        "    \n",
        "    \n",
        "    print(\" - Handling Dates\")\n",
        "    # Convert dates to datetime\n",
        "    data['VerifiedDate'] = pd.to_datetime(data['VerifiedDate'], errors = 'coerse')\n",
        "    # If there is a valid date, create a binary flag\n",
        "    data['Verified'] = 'False'\n",
        "    data.loc[data['VerifiedDate'].notnull(), 'Verified'] = 'True'\n",
        "    # Get date from datetime\n",
        "    data['VerifiedDate'] = data.pop('VerifiedDate').dt.strftime(\"%Y-%m-%d\")\n",
        "    \n",
        "    \n",
        "    \n",
        "    if location_type == 'RVs':\n",
        "        \n",
        "        print(\" - Road Access Type\")\n",
        "        # Read in Open Roads data within a distance of each RV as BNG\n",
        "        road_dist = 50\n",
        "        with warnings.catch_warnings():\n",
        "            # Handle a bug when reading files\n",
        "            warnings.filterwarnings('ignore', message = \"Sequential read of iterator was interrupted\")\n",
        "            roads = gpd.read_file(openroads_filepath,\n",
        "                                  mask = point_buffer(data, road_dist, \"EPSG:27700\"))\n",
        "        roads.crs = \"EPSG:27700\"\n",
        "        # Keep only necessary columns\n",
        "        roads = roads[['roadFunction', 'geometry']]\n",
        "        \n",
        "        # Clip roads to the buffer and convert to single part\n",
        "        roads = gpd.clip(roads, point_buffer(data, road_dist, \"EPSG:27700\")).explode(index_parts = True)\n",
        "        # Resample the road vertices\n",
        "        roads['geometry'] = roads.geometry.apply(redistribute_vertices, distance = 2)\n",
        "        \n",
        "        # Extract the coordinates of each vertex for each line as points with their road access type\n",
        "        road_type_points = []\n",
        "        for index, row in roads.iterrows():\n",
        "            coords = [i for i in row['geometry'].coords]\n",
        "            temp = gpd.GeoDataFrame(crs = \"EPSG:27700\",\n",
        "                                    geometry = gpd.points_from_xy([x for x, y in coords],\n",
        "                                                                  [y for x, y in coords]))\n",
        "            temp['RoadAccessType'] = row['roadFunction']\n",
        "            road_type_points.append(temp)\n",
        "        road_type_points = gpd.GeoDataFrame(pd.concat(road_type_points,\n",
        "                                                      ignore_index = True),\n",
        "                                            crs = road_type_points[0].crs)\n",
        "        # Get the closest road type point and distance in metres\n",
        "        data = get_nearest(data, road_type_points)\n",
        "        data['RoadDistanceMetres'] = data.pop('dist').round(0).astype(int)\n",
        "        # Remove roads further than the buffer distance\n",
        "        data.loc[data['RoadDistanceMetres'] > road_dist, 'RoadAccessType'] = 'Unknown'\n",
        "        # Remove the distance column\n",
        "        del data['RoadDistanceMetres']\n",
        "        \n",
        "        \n",
        "        \n",
        "        print(\" - Postcode\")\n",
        "        # Read in Code Point data within a distance of each RV as BNG\n",
        "        postcode_dist = 300\n",
        "        with warnings.catch_warnings():\n",
        "            # Handle a bug when reading files\n",
        "            warnings.filterwarnings('ignore', message = \"Sequential read of iterator was interrupted\")\n",
        "            postcodes = gpd.read_file(codepoint_filepath,\n",
        "                                      mask = point_buffer(data, postcode_dist, \"EPSG:27700\"))\n",
        "        postcodes.crs = \"EPSG:27700\"\n",
        "        # Keep only necessary columns\n",
        "        postcodes = postcodes[['Postcode', 'geometry']]\n",
        "        # Remove spaces\n",
        "        postcodes['Postcode'] = postcodes['Postcode'].str.replace(' ', '')\n",
        "        # Get the closest postcode and distance in metres\n",
        "        data = get_nearest(data, postcodes)\n",
        "        data['PostcodeDistanceMetres'] = data.pop('dist').round(0).astype(int)\n",
        "        # Remove postcodes further than the buffer distance\n",
        "        data.loc[data['PostcodeDistanceMetres'] > postcode_dist, 'Postcode'] = 'None'\n",
        "        # Remove the distance column\n",
        "        del data['PostcodeDistanceMetres']\n",
        "        \n",
        "        \n",
        "        \n",
        "        print(\" - Mobile Phone Coverage\")\n",
        "        # All providers\n",
        "        providers = {'EE': 'EE',\n",
        "                     'H3': 'Three',\n",
        "                     'VO': 'Vodafone',\n",
        "                     'TF': 'O2'}\n",
        "        # Scores and meaning\n",
        "        coverage_scores = {0: 'Black',  # No signal predicted\n",
        "                           1: 'Red',    # Reliable signal unlikely\n",
        "                           2: 'Amber',  # May experience problems with connectivity\n",
        "                           3: 'Green',  # Likely to have good coverage and receive a basic data rate\n",
        "                           4: 'Blue'}   # Likely to have good coverage indoors and to receive an enhanced data rate\n",
        "        \n",
        "        # Create the postcode-coverage lookup\n",
        "        mobile_coverage = {}\n",
        "        for postcode in list(set(data['Postcode'])):\n",
        "            time.sleep(0.15) # < 500 calls/minute limit\n",
        "            coverage = get_coverage(postcode)\n",
        "            # If we get results back\n",
        "            if coverage:\n",
        "                provider_results = []\n",
        "                # Get the lowest score for each provider in the postcode for outdoor voice calls without 4G\n",
        "                for provider in providers.keys():\n",
        "                    provider_name = providers.get(provider)\n",
        "                    score = min([i.get(provider + 'VoiceOutdoorNo4g') for i in coverage])\n",
        "                    # Check that the returned score is valid\n",
        "                    if score not in coverage_scores.keys():\n",
        "                        raise Exception('Coverage score ' + str(score) + ' is not valid')\n",
        "                    # Format the provider result and add it to the list\n",
        "                    provider_results.append(provider_name + \" (\" + coverage_scores.get(score) + \")\")\n",
        "                # Concatenate the provider results and assign to the postcode\n",
        "                mobile_coverage[postcode] = \", \".join(provider_results)\n",
        "            # If we don't get results back\n",
        "            else:\n",
        "                mobile_coverage[postcode] = 'Unknown'\n",
        "                \n",
        "        # Lookup the coverage for each postcode\n",
        "        data['MobileCoverage'] = [mobile_coverage.get(i) for i in data['Postcode']]\n",
        "    \n",
        "    \n",
        "    \n",
        "    # Regardless of location type\n",
        "    \n",
        "    print(\" - Longitude, Latitude\")\n",
        "    data['Longitude'] = data.pop('Longitude')\n",
        "    data['Latitude'] = data.pop('Latitude')\n",
        "    \n",
        "    print(\" - Easting, Northing\")\n",
        "    data['Easting'] = data['geometry'].x.astype(int)\n",
        "    data['Northing'] = data['geometry'].y.astype(int)\n",
        "\n",
        "    print(\" - OS Grid Reference\")\n",
        "    data['OSGridRef1m'] = data.apply(lambda x: xy_to_osgb(x['geometry'].x, x['geometry'].y), axis = 1)\n",
        "\n",
        "    print(\" - What3Words\")\n",
        "    data['What3Words'] = data.apply(lambda x: get_what3words(x['Latitude'], x['Longitude']), axis = 1)\n",
        "    \n",
        "    print(\" - Google Maps URL\")\n",
        "    # https://developers.google.com/maps/documentation/urls/get-started\n",
        "    data['GoogleMapsURL'] = \"https://www.google.com/maps/search/?api=1&query=\" + data['Latitude'].astype(str) + \"%2C\" + data['Longitude'].astype(str)\n",
        "    \n",
        "    \n",
        "    \n",
        "    print(\" - Exporting Files\")\n",
        "    # Replace blanks with none\n",
        "    data = data.replace(np.nan, 'None').replace('NaT', 'None')\n",
        "    \n",
        "    # Reset index and name as fid\n",
        "    data = data.reset_index(drop = True)\n",
        "    data.index.name = 'fid'\n",
        "    # Start the index at 1\n",
        "    data.index += 1 \n",
        "    \n",
        "    # Export to BNG GeoPackage\n",
        "    data.to_file(os.path.join(export_directory,\n",
        "                              location_type + \"_BNG.gpkg\"),\n",
        "                 layer = location_type + \"_BNG\",\n",
        "                 driver = 'GPKG')\n",
        "    \n",
        "    # Convert to WGS84 using the original longitude and latitude\n",
        "    data = gpd.GeoDataFrame(data,\n",
        "                            crs = \"EPSG:4326\",\n",
        "                            geometry = gpd.points_from_xy(data['Longitude'],\n",
        "                                                          data['Latitude']))\n",
        "    \n",
        "    # Export to WGS84 GeoPackage\n",
        "    data.to_file(os.path.join(export_directory,\n",
        "                              location_type + \"_WGS84.gpkg\"),\n",
        "                 layer = location_type + \"_WGS84\",\n",
        "                 driver = 'GPKG')\n",
        "    \n",
        "    # Remove the geometry column\n",
        "    del data['geometry']\n",
        "    \n",
        "    # Export to CSV\n",
        "    data.to_csv(os.path.join(export_directory,\n",
        "                             location_type + \".csv\"),\n",
        "                index = False)\n",
        "    \n",
        "    \n",
        "\n",
        "    # Build a GPX file\n",
        "    # Loop through each row and concatenate columns\n",
        "    descriptions = []\n",
        "    for i, row in data.iterrows():\n",
        "        descriptions.append(\" | \".join([str(row.index.values[i]) + \": \" + str(row.values[i]) for i in range(len(row.index.values))]))\n",
        "    data['Concat'] = descriptions\n",
        "    \n",
        "    # Create a blank GPX file\n",
        "    gpx = g.GPX()\n",
        "    gpx.creator = \"github.com/EdwardALockhart/SpatialDataIncidentResponse\"\n",
        "    \n",
        "    # Loop through each location record\n",
        "    for i, row in data.iterrows():\n",
        "        # Create a blank waypoint\n",
        "        wpt = gpxpy.gpx.GPXWaypoint()\n",
        "        \n",
        "        # Add data to the waypoint\n",
        "        wpt.name = row['Name']\n",
        "        wpt.longitude = row['Longitude']\n",
        "        wpt.latitude = row['Latitude']\n",
        "        wpt.comment = row['Concat']\n",
        "        wpt.desciption = row['Concat']\n",
        "        if location_type == 'RVs' and rv_gpx_symbology != 'None':\n",
        "            wpt.symbol = rv_gpx_symbology\n",
        "        elif location_type == 'APs' and ap_gpx_symbology != 'None':\n",
        "            wpt.symbol = ap_gpx_symbology\n",
        "            \n",
        "        # Add to the GPX file\n",
        "        gpx.waypoints.append(wpt)\n",
        "    \n",
        "    # Export to WGS84 GPX\n",
        "    with open(os.path.join(export_directory,\n",
        "                           location_type + \"_\" + date + \"_WGS84.gpx\"),\n",
        "              'w', encoding = \"UTF-8\") as file:\n",
        "        file.write(gpx.to_xml(version = \"1.1\"))\n",
        "\n",
        "    print(\" - Done\")\n",
        "\n",
        "\n",
        "\n",
        "print(\"\\nComplete\")"
      ],
      "metadata": {
        "id": "sWgy1otAw2XL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
